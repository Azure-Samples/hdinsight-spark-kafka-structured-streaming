{"nbformat_minor": 2, "cells": [{"source": "## To use this notebook\n\nJupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n\nNOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n\n## Requirements\n\n* An Azure Virtual Network\n* A Spark on HDInsight 3.6 cluster, inside the virtual network\n* A Kafka on HDInsight cluster, inside the virtual network\n\n## Load packages\n\nTo use Spark structured streaming with Kafka, you must load the spark-sql-kafka package. The version must match the version of both kafka and Spark that you are using. The name of the package contains the versions that it works with. For example, `spark-sql-kafka-0-10_2.11:2.1.0` works with the following versions:\n\n* Kafka 0.10\n* Spark 2.1.0\n* Scala 2.11\n\nRun the next cell to load a package that works with Kafka on HDInsight 3.6, and Spark 2.1 on HDInsight 3.6.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0\", \n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Define a schema for the data\nWhen reading data from Kafka, the data is provided in the 'value' column. In this example, the data is a JSON document that describes a Tweet. Run the following cell to create a schema for the JSON document structure.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Import bits useed for declaring schemas and working with JSON data\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Define the structure of the Twitter JSON document that is read from Kafka\n// Note, this isn't pretty, but there's some odd behavior where moving .add to \n// a new line causes an error.\nval schema = (new StructType).add(\"created_at\", StringType).add(\"id\", LongType).add(\"id_str\", StringType).add(\"text\", StringType).add(\"source\", StringType).add(\"truncated\", BooleanType).add(\"in_reply_to_status_id\", LongType).add(\"in_reply_to_status_id_str\", StringType).add(\"in_reply_to_user_id\", LongType).add(\"in_reply_to_user_id_str\", StringType).add(\"in_reply_to_screen_name\", StringType).add(\"user\", (new StructType).add(\"id\", LongType)\n        .add(\"id_str\", StringType)\n        .add(\"name\", StringType)\n        .add(\"screen_name\", StringType)\n        .add(\"location\", StringType)\n        .add(\"url\", StringType)\n        .add(\"description\", StringType)\n        .add(\"protected\", BooleanType)\n        .add(\"verified\", BooleanType)\n        .add(\"followers_count\", LongType)\n        .add(\"friends_count\", LongType)\n        .add(\"listed_count\", LongType)\n        .add(\"favourites_count\", LongType)\n        .add(\"statuses_count\", LongType)\n        .add(\"created_at\", StringType)\n        .add(\"utc_offset\", IntegerType)\n        .add(\"time_zone\", StringType)\n        .add(\"geo_enabled\", BooleanType)\n        .add(\"lang\", StringType)\n        .add(\"contributors_enabled\", BooleanType)\n        .add(\"is_translator\", BooleanType)\n        .add(\"profile_background_color\", StringType)\n        .add(\"profile_background_image_url\", StringType)\n        .add(\"profile_background_image_url_https\", StringType)\n        .add(\"profile_background_tile\", BooleanType)\n        .add(\"profile_link_color\", StringType)\n        .add(\"profile_sidebar_border_color\", StringType)\n        .add(\"profile_sidebar_fill_color\", StringType)\n        .add(\"profile_text_color\", StringType)\n        .add(\"profile_use_background_image\", BooleanType)\n        .add(\"profile_image_url\", StringType)\n        .add(\"profile_image_url_https\", StringType)\n        .add(\"profile_banner_url\", StringType)\n        .add(\"default_profile\", BooleanType)\n        .add(\"default_profile_image\", BooleanType)\n        .add(\"following\", StringType)\n        .add(\"follow_request_sent\", StringType)\n        .add(\"notifications\", StringType)).add(\"geo\", StringType).add(\"coordinates\", StringType).add(\"place\", StringType).add(\"contributors\", StringType).add(\"is_quote_status\", BooleanType).add(\"retweet_count\", LongType).add(\"favorite_count\", LongType).add(\"entities\", (new StructType)\n        .add(\"hashtags\", ArrayType((new StructType)\n            .add(\"text\", StringType)\n            .add(\"indices\", ArrayType(LongType)))).add(\"urls\", ArrayType((new StructType)\n            .add(\"url\", StringType)\n            .add(\"expanded_url\", StringType)\n            .add(\"display_url\", StringType)\n            .add(\"indices\", ArrayType(LongType))))\n        .add(\"user_mentions\", ArrayType(StringType))\n        .add(\"symbols\", ArrayType(StringType))).add(\"favorited\", BooleanType).add(\"retweeted\", BooleanType).add(\"possibly_sensitive\", BooleanType).add(\"filter_level\", StringType).add(\"lang\", StringType).add(\"timestamp_ms\", StringType)\n\n// Uncomment to see a tree view of the schema.\n//schema.printTreeString", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Read the data and apply the schema\n\nIn the following cell, replace `YOUR_KAFKA_BROKER_HOSTS` with the broker hosts for your Kafka cluster. To get the broker host information, use one of the following methods:\n\n* From __Bash__ or other Unix shell:\n\n    ```bash\ncurl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2\n    ```\n    \n    Note: This assumes that `$PASSWORD` is set to the password for your HDInsight cluster admin, and that `$CLUSTERNAME` is set to the name of the cluster.\n\n    * From __Azure Powershell__:\n\n    ```powershell\n$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n    -Credential $creds\n$respObj = ConvertFrom-Json $resp.Content\n$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n($brokerHosts -join \":9092,\") + \":9092\"\n    ```\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Read from the Kafka stream source\nval kafka = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"YOUR_KAFKA_BROKER_HOSTS\").option(\"subscribe\", \"tweets\").option(\"startingOffsets\",\"earliest\").load()\n\n/* Select the following columns from the Kafka data:\n   * value - the JSON data for a tweet\n   Use from_json to apply the schema and store the schematized data in the 'tweet' column\n*/\nval tweetData=kafka.select(\n    from_json(col(\"value\").cast(\"string\"), schema) as \"tweet\")\n\n// There's a lot of data in the Twitter JSON object. Just grab the tweet ID, user name, and text\nval tweetText=tweetData.select(\"tweet.id\",\n                               \"tweet.user.name\",\n                               \"tweet.text\")\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Process the stream\n\nTo start processing the stream, write it to a sink. Run the following cell to write the data to the console (cell output). This cell runs for 30 seconds, then displays the results.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Start writing the stream to the console. Use a timeout so that control is returned to the notebook.\ntweetText.writeStream.format(\"console\").start.awaitTermination(30000)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}, "anaconda-cloud": {}}}