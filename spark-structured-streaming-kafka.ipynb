{"nbformat_minor": 2, "cells": [{"source": "# Spark structured streaming with Kafka on HDInsight\n\nThis notebook demonstrates how to use Spark Structured Streaming with Kafka on HDInsight. It uses data on taxi trips, which is provided by New York City.\n\nThe data set used by this notebook is from [2016 Green Taxi Trip Data](https://data.cityofnewyork.us/Transportation/2016-Green-Taxi-Trip-Data/hvrh-b6nb).\n\n## To use this notebook\n\nJupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n\nNOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n\n## Requirements\n\n* An Azure Virtual Network\n* A Spark (2.2.0) on HDInsight 3.6 cluster, inside the virtual network\n* A Kafka on HDInsight 3.6 cluster, inside the virtual network\n\n## Load packages\n\nRun the next cell to load packages used by this notebook:\n\n* spark-streaming-kafka-0-8_2.10, version 2.2.0 - Used to read and write data with Kafka.\n\n__NOTE__: The first time you run this block, it may take a minute or longer. This happens because the Spark cluster must retrieve the packages from the Maven repository on the internet.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1522866477790_0008</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ldf1sp.ufreppehjbjuhg0dicapk3bl5h.jx.internal.cloudapp.net:8088/proxy/application_1522866477790_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-ldf1sp.ufreppehjbjuhg0dicapk3bl5h.jx.internal.cloudapp.net:30060/node/containerlogs/container_1522866477790_0008_01_000001/livy\">Link</a></td><td></td></tr><tr><td>7</td><td>application_1522866477790_0012</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ldf1sp.ufreppehjbjuhg0dicapk3bl5h.jx.internal.cloudapp.net:8088/proxy/application_1522866477790_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-ldf1sp.ufreppehjbjuhg0dicapk3bl5h.jx.internal.cloudapp.net:30060/node/containerlogs/container_1522866477790_0012_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "## Create the Kafka topic\n\nIn the next cell, you must provide the Zookeeper host information for your Kafka cluster. Use the following steps to get this information:\n\n* From __Bash__ or other Unix shell:\n\n    ```bash\nCLUSTERNAME='the name of your HDInsight cluster'\nPASSWORD='the password for your cluster login account'\ncurl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2\n    ```\n\n* From __Azure PowerShell__:\n\n    ```powershell\n$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" `\n    -Credential $creds `\n    -UseBasicParsing\n$respObj = ConvertFrom-Json $resp.Content\n$zkHosts = $respObj.host_components.HostRoles.host_name[0..1]\n($zkHosts -join \":2181,\") + \":2181\"\n    ````\n\nThe return value is similar to the following example:\n\n`zk0-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181,zk1-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181`\n\nReplace the `YOUR_ZOOKEEPER_HOSTS` in the next cell with the returned value, and then run the cell", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash \n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic tripdata --zookeeper 'YOUR_ZOOKEEPER_HOSTS'", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Retrieve data on taxi trips\n\nRun the next cell to load data on taxi trips in New York City. The data is loaded into a dataframe and then the dataframe is displayed as the cell output.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "import spark.implicits._\n// Load the data from the New York City Taxi data REST API for 2016 Green Taxi Trip Data\nval url=\"https://data.cityofnewyork.us/resource/pqfs-mqru.json\"\nval result = scala.io.Source.fromURL(url).mkString\n\n// Create a dataframe from the JSON data\nval taxiDF = spark.read.json(Seq(result).toDS)\n\n// Display the dataframe containing trip data\ntaxiDF.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1522866477790_0013</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ldf1sp.ufreppehjbjuhg0dicapk3bl5h.jx.internal.cloudapp.net:8088/proxy/application_1522866477790_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-ldf1sp.ufreppehjbjuhg0dicapk3bl5h.jx.internal.cloudapp.net:30060/node/containerlogs/container_1522866477790_0013_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\n|  dropoff_latitude|  dropoff_longitude|extra|fare_amount|improvement_surcharge|lpep_dropoff_datetime|lpep_pickup_datetime|mta_tax|passenger_count|payment_type|   pickup_latitude|   pickup_longitude|ratecodeid|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|trip_type|vendorid|\n+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\n|40.698043823242188|-73.924278259277344|  0.5|          8|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:29:...|    0.5|              1|           1|40.680610656738281|-73.928642272949219|         1|                 N|      1.86|           0|       11.16|         1.46|        1|       2|\n|40.761379241943359|-73.923919677734375|  0.5|       15.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:19:...|    0.5|              1|           2|40.723175048828125|-73.952674865722656|         1|                 N|         0|           0|        16.8|         3.56|        1|       2|\n|40.646072387695313|-74.013160705566406|  0.5|       16.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:19:...|    0.5|              1|           1|40.676105499267578|-73.971611022949219|         1|                 N|      4.45|           0|       22.25|         3.79|        1|       2|\n|40.689033508300781|-74.000648498535156|  0.5|       13.5|                  0.3| 2016-01-01T00:38:...|2016-01-01T00:22:...|    0.5|              1|           2|40.669578552246094|   -73.989501953125|         1|                 N|         0|           0|        14.8|         3.01|        1|       2|\n|40.663013458251953|-73.940719604492188|  0.5|         12|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:24:...|    0.5|              1|           2|40.682853698730469|-73.964729309082031|         1|                 N|         0|           0|        13.3|         2.55|        1|       2|\n|40.742111206054688|-73.867744445800781|  0.5|          7|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:32:...|    0.5|              1|           2|40.746456146240234|-73.891143798828125|         1|                 N|         0|           0|         8.3|         1.37|        1|       2|\n|40.745689392089844|-73.886192321777344|  0.5|          5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:34:...|    0.5|              1|           2|40.746196746826172|-73.896675109863281|         1|                 N|         0|           0|         6.3|         0.57|        1|       2|\n|40.794120788574219|-73.949150085449219|  0.5|          7|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:31:...|    0.5|              1|           2|40.803558349609375|-73.953353881835937|         1|                 N|         0|           0|         8.3|         1.01|        1|       2|\n|40.679725646972656|-73.971572875976562|  0.5|         12|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:24:...|    0.5|              1|           1|40.702816009521484|-73.994064331054688|         1|                 N|         2|           0|        15.3|         2.46|        1|       2|\n|40.739658355712891|-73.917549133300781|  0.5|          9|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:28:...|    0.5|              1|           1|40.756641387939453|-73.914131164550781|         1|                 N|       1.6|           0|        11.9|         1.61|        1|       2|\n|40.763126373291016|-73.921028137207031|  0.5|          6|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:32:...|    0.5|              1|           1|40.761829376220703|-73.911178588867188|         1|                 N|      1.82|           0|        9.12|         0.72|        1|       2|\n|40.718177795410156|-73.962753295898438|  0.5|        3.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:37:...|    0.5|              1|           1|40.715328216552734|-73.958168029785156|         1|                 N|      0.96|           0|        5.76|         0.32|        1|       2|\n|40.842765808105469|-73.924903869628906|  0.5|       14.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:21:...|    0.5|              1|           2|40.800785064697266|-73.946678161621094|         1|                 N|         0|           0|        15.8|         3.54|        1|       2|\n|40.775833129882812| -73.90240478515625|  0.5|          6|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:34:...|    0.5|              1|           2|40.763439178466797|-73.914291381835938|         1|                 N|         0|           0|         7.3|         1.10|        1|       2|\n|40.850196838378906|-73.932029724121094|  0.5|         11|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:26:...|    0.5|              1|           1|40.824314117431641|-73.943374633789063|         1|                 N|         2|           0|        14.3|         2.28|        1|       2|\n|40.640159606933594|-73.966880798339844|  0.5|        4.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:35:...|    0.5|              1|           1|40.632152557373047| -73.96697998046875|         1|                 N|      1.16|           0|        6.96|         0.68|        1|       2|\n|40.811866760253906|-73.951583862304688|  0.5|         10|                  0.3| 2016-01-01T00:38:...|2016-01-01T00:25:...|    0.5|              1|           2|40.814445495605469|-73.937843322753906|         1|                 N|         0|           0|        11.3|         1.36|        1|       2|\n|40.693325042724609|-73.947746276855469|  0.5|       15.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:17:...|    0.5|              1|           1|40.690513610839844|-73.990364074707031|         1|                 N|         3|           0|        19.8|         3.07|        1|       2|\n|40.751564025878906|-73.855522155761719|  0.5|        7.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:31:...|    0.5|              1|           2|    40.74755859375|-73.883827209472656|         1|                 N|         0|           0|         8.8|         1.52|        1|       2|\n|40.659637451171875|-73.976943969726563|  0.5|       11.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:25:...|    0.5|              1|           2|40.684413909912109|-73.980354309082031|         1|                 N|         0|           0|        12.8|         2.55|        1|       2|\n+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "## Set the Kafka broker hosts information\n\nIn the next cell, replace YOUR_KAFKA_BROKER_HOSTS with the broker hosts for your Kafka cluster. This is used to write data to the Kafka cluster. To get the broker host information, use one of the following methods:\n\n* From Bash or other Unix shell:\n\n    ```bash\nCLUSTERNAME='the name of your HDInsight cluster'\nPASSWORD='the password for your cluster login account'\ncurl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2\n    ```\n\n* From Azure Powershell:\n\n    ```powershell\n$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n  -Credential $creds -UseBasicParsing\n$respObj = ConvertFrom-Json $resp.Content\n$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n($brokerHosts -join \":9092,\") + \":9092\"\n    ```", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"YOUR_KAFKA_BROKER_HOSTS\"\nval kafkaTopic=\"tripdata\"\n\nprintln(\"Finished setting Kafka broker and topic configuration.\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Finished setting Kafka broker and topic configuration."}], "metadata": {"collapsed": false}}, {"source": "## Send the data to Kafka\n\nIn the following cell, the `vendorid` field is used as the key value for the Kafka message. The key is used by Kafka when partitioning data. All of the fields are stored in the Kafka message as a JSON string value.\n\nRun the following cell save the data to Kafka using a batch query. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "// Select the vendorid as the key and save the JSON string as the value.\nval query = taxiDF.selectExpr(\"CAST(vendorid AS STRING) as key\", \"to_JSON(struct(*)) AS value\").write.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"topic\", kafkaTopic).save()\nprintln(\"Data sent to Kafka\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Data sent to Kafka"}], "metadata": {"collapsed": false}}, {"source": "## Declare a schema\n\nThe following cell demonstrates how to use a schema when reading JSON data from kafka.", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "// Import bits useed for declaring schemas and working with JSON data\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Define a schema for the data\nval schema = (new StructType).add(\"dropoff_latitude\", StringType).add(\"dropoff_longitude\", StringType).add(\"extra\", StringType).add(\"fare_amount\", StringType).add(\"improvement_surcharge\", StringType).add(\"lpep_dropoff_datetime\", StringType).add(\"lpep_pickup_datetime\", StringType).add(\"mta_tax\", StringType).add(\"passenger_count\", StringType).add(\"payment_type\", StringType).add(\"pickup_latitude\", StringType).add(\"pickup_longitude\", StringType).add(\"ratecodeid\", StringType).add(\"store_and_fwd_flag\", StringType).add(\"tip_amount\", StringType).add(\"tolls_amount\", StringType).add(\"total_amount\", StringType).add(\"trip_distance\", StringType).add(\"trip_type\", StringType).add(\"vendorid\", StringType)\n// Reproduced here for readability\n//val schema = (new StructType)\n//   .add(\"dropoff_latitude\", StringType)\n//   .add(\"dropoff_longitude\", StringType)\n//   .add(\"extra\", StringType)\n//   .add(\"fare_amount\", StringType)\n//   .add(\"improvement_surcharge\", StringType)\n//   .add(\"lpep_dropoff_datetime\", StringType)\n//   .add(\"lpep_pickup_datetime\", StringType)\n//   .add(\"mta_tax\", StringType)\n//   .add(\"passenger_count\", StringType)\n//   .add(\"payment_type\", StringType)\n//   .add(\"pickup_latitude\", StringType)\n//   .add(\"pickup_longitude\", StringType)\n//   .add(\"ratecodeid\", StringType)\n//   .add(\"store_and_fwd_flag\", StringType)\n//   .add(\"tip_amount\", StringType)\n//   .add(\"tolls_amount\", StringType)\n//   .add(\"total_amount\", StringType)\n//   .add(\"trip_distance\", StringType)\n//   .add(\"trip_type\", StringType)\n//   .add(\"vendorid\", StringType)\n\nprintln(\"Schema declared\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Schema declared"}], "metadata": {"collapsed": false}}, {"source": "## Select data and start the stream\n\nThe following cell demonstrates how to retrieve data from kafka using a batch query, and then write the results out to HDFS on the Spark cluster. In this example, the select retrieves the message (`value` field) from Kafka and applies the schema to it. The data is then written to HDFS (WASB or ADL) in parquet format.\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "// Read a batch from Kafka\nval kafkaDF = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"subscribe\", kafkaTopic).option(\"startingOffsets\", \"earliest\").load()\n// Select data and write to file\nval query = kafkaDF.select(from_json(col(\"value\").cast(\"string\"), schema) as \"trip\").write.format(\"parquet\").option(\"path\",\"/example/batchtripdata\").option(\"checkpointLocation\", \"/batchcheckpoint\").save()\nprintln(\"Wrote data to file\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Wrote data to file"}], "metadata": {"collapsed": false}}, {"source": "You can verify that the files were created by running the following cell. It lists the files in the /example/tripdata directory.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /example/batchtripdata", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 9 items\n-rw-r--r--   1 livy supergroup          0 2018-04-05 19:06 /example/batchtripdata/_SUCCESS\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:06 /example/batchtripdata/part-00000-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:06 /example/batchtripdata/part-00001-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:06 /example/batchtripdata/part-00002-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:06 /example/batchtripdata/part-00003-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:06 /example/batchtripdata/part-00004-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:06 /example/batchtripdata/part-00005-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup      75475 2018-04-05 19:06 /example/batchtripdata/part-00006-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:06 /example/batchtripdata/part-00007-7b04ccb7-e0cb-46b2-8285-2d26ca6eabd3-c000.snappy.parquet\n"}], "metadata": {"collapsed": false}}, {"source": "While the previous example used a batch query, the following cell demonstrates how to do the same thing using a streaming query. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "// Stream from Kafka\nval kafkaStreamDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"subscribe\", kafkaTopic).option(\"startingOffsets\", \"earliest\").load()\n// Select data from the stream and write to file\nkafkaStreamDF.select(from_json(col(\"value\").cast(\"string\"), schema) as \"trip\").writeStream.format(\"parquet\").option(\"path\",\"/example/streamingtripdata\").option(\"checkpointLocation\", \"/streamcheckpoint\").start.awaitTermination(30000)\nprintln(\"Wrote data to file\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Wrote data to file"}], "metadata": {"collapsed": false}}, {"source": "Run the following cell to verify that the files were written by the streaming query.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /example/streamingtripdata", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 9 items\ndrwxr-xr-x   - livy supergroup          0 2018-04-05 19:07 /example/streamingtripdata/_spark_metadata\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:07 /example/streamingtripdata/part-00000-2d412a48-069f-4337-8e09-91d9696953ee-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:07 /example/streamingtripdata/part-00001-d54b3575-a3dd-46dd-9f9c-86500eacbedd-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:07 /example/streamingtripdata/part-00002-bfbb884e-b4af-4ef4-bc6b-f30ed0760e1f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:07 /example/streamingtripdata/part-00003-00e5d3e6-b42c-4822-a05a-2b82d2b15c3b-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:07 /example/streamingtripdata/part-00004-6e91284f-12e7-4a8c-8742-e8b803516329-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:07 /example/streamingtripdata/part-00005-9cd6dafd-4ac2-4031-96b1-a71128163500-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2018-04-05 19:07 /example/streamingtripdata/part-00006-1e0e842e-0791-4dcc-a63f-dfab51edc2d4-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup      69270 2018-04-05 19:07 /example/streamingtripdata/part-00007-c2cbd74f-5695-4158-aadc-4a09882428ee-c000.snappy.parquet\n"}], "metadata": {"collapsed": false}}, {"source": "For more information, see the [Structured Streaming + Kafka integration guide](https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html).", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}